**Proyecto: Procesamiento de Logs con Apache Airflow y Spark
隆Bienvenido al repositorio de mi proyecto "Procesamiento de Logs con Apache Airflow y Spark"! 
Este proyecto es una soluci贸n end-to-end para procesar y analizar archivos de logs a gran escala, utilizando tecnolog铆as modernas de orquestaci贸n (Airflow) 
y procesamiento distribuido (Spark). 
Est谩 dise帽ado para demostrar habilidades t茅cnicas avanzadas, resoluci贸n de problemas y capacidad para trabajar con big data en un entorno de contenedores Docker. 

 Descripci贸n General
Este proyecto automatiza el procesamiento de un archivo de logs (access.log, el mismo es muy pesado para adjuntarlo en el repo, por lo que tuve que adjuntar el link de descarga del mismo)
utilizando Apache Spark para an谩lisis distribuidos y Apache Airflow para la orquestaci贸n de tareas. El flujo completo incluye:

Lectura y transformaci贸n de logs con Spark.
Generaci贸n de un informe con estad铆sticas (c贸digos de estado, errores cr铆ticos, etc.).
Env铆o de un email con el informe y los datos procesados. Todo esto se ejecuta en un entorno Dockerizado, lo que asegura portabilidad y escalabilidad.
 Objetivo
Demostrar la integraci贸n de herramientas de big data (Spark) y orquestaci贸n (Airflow).
Automatizar el an谩lisis de logs para identificar patrones y anomal铆as.
Crear un pipeline reproducible para uso en entornos reales o pruebas t茅cnicas.

 Tecnolog铆as Utilizadas
Apache Airflow	Orquestador de workflows para automatizar tareas.
Apache Spark	Motor de procesamiento distribuido para big data.
Docker	Contenedorizaci贸n para entornos consistentes.
Python	Lenguaje principal para scripts y an谩lisis.
Pandas	An谩lisis de datos y visualizaci贸n en reportes.
GitHub	Control de versiones y despliegue del c贸digo.
PostgreSQL	Base de datos para metadatos de Airflow (opcional).

 Flujo End-to-End del Proyecto
Ingesti贸n de Datos:
Se parte de un archivo access.log que simula logs de un servidor web.
Este archivo se monta en un volumen Docker para ser accesible por Spark.
Procesamiento con Spark:
Un script Python (process_logs_spark.py) usa Spark para leer el access.log, particionarlo, y generar un archivo status_codes.csv con estad铆sticas (c贸digos de estado, conteos, etc.).
Ejecuci贸n dentro de un contenedor Spark via docker exec -it.
Orquestaci贸n con Airflow:
Un DAG (process_logs_dag.py) define dos tareas:
run_spark_script: Ejecuta el script de Spark.
generate_and_send_report: Genera un informe y lo env铆a por email.
Airflow gestiona la ejecuci贸n, dependencias y reintentos.
Generaci贸n y Env铆o de Reportes:
La tarea generate_and_send_report lee status_codes.csv, analiza datos (c贸digos cr铆ticos, picos de errores), y env铆a un email con el informe y el archivo adjunto usando SMTP.
Despliegue:
Todo se ejecuta en un entorno Docker compuesto por servicios (Airflow, Spark, Postgres).
Se parte de dos archivos dockerfile-airflow/spark y luego un yaml, para levantar los servicios de airflow.
El proyecto se inicia con docker-compose up -d --build(creando las im谩genes y levantando los contenedores).

 Inconvenientes Potenciales y Soluciones
Inconveniente	Causa	Soluci贸n
Archivos grandes en GitHub	Logs o resultados superan 100 MB.	Usar .gitignore para excluir data/ y limpiar el historial con git filter-branch.
Errores de email (SMTP)	Credenciales inv谩lidas o puerto incorrecto.	Verificar .env con contrase帽a de aplicaci贸n de Gmail y puerto 587.
Rutas de archivos inconsistentes	Diferencias entre contenedores.	Asegurar que vol煤menes Docker mapeen correctamente (ej. ./data:/opt/airflow/data).
Fallo en Spark	Configuraci贸n de Java o Spark mal definida.	Verificar JAVA_HOME y SPARK_HOME en el contenedor.
Ejecutar docker exec -it directamente desde el contenedor de airflow, asegurarse de a帽adir socket de docker en contenedores webserver y scheduler.

 Habilidades Desarrolladas
Desarrollo: Integraci贸n de Airflow, Spark y Docker.
Resoluci贸n de problemas: Manejo de errores de tama帽o de archivos, rutas y autenticaci贸n.
Automatizaci贸n: Pipelines de datos end-to-end.

 Contacto
GitHub: Mercado-tomas
Email: tomasfmercado@gmail.com
隆Estoy abierto a oportunidades y colaboraciones!
